{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac1bb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hi\\anaconda3\\envs\\all\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting DQN Training ---\n",
      "Episode 1/200: Final Assets = 135,567.06 USD, Reward = 11.31, Epsilon = 0.9913, Steps = 502\n",
      "Episode 2/200: Final Assets = 88,899.71 USD, Reward = 6.95, Epsilon = 0.9814, Steps = 502\n",
      "Episode 3/200: Final Assets = 119,958.06 USD, Reward = 7.66, Epsilon = 0.9714, Steps = 502\n",
      "Episode 4/200: Final Assets = 87,818.38 USD, Reward = 8.20, Epsilon = 0.9615, Steps = 502\n",
      "Episode 5/200: Final Assets = 109,508.02 USD, Reward = 10.07, Epsilon = 0.9515, Steps = 502\n",
      "Episode 6/200: Final Assets = 102,982.24 USD, Reward = 6.29, Epsilon = 0.9416, Steps = 502\n",
      "Episode 7/200: Final Assets = 104,707.49 USD, Reward = 8.29, Epsilon = 0.9317, Steps = 502\n",
      "Episode 8/200: Final Assets = 95,775.45 USD, Reward = 6.72, Epsilon = 0.9217, Steps = 502\n",
      "Episode 9/200: Final Assets = 77,074.96 USD, Reward = 6.84, Epsilon = 0.9118, Steps = 502\n",
      "Episode 10/200: Final Assets = 149,617.68 USD, Reward = 11.34, Epsilon = 0.9019, Steps = 502\n",
      "Episode 11/200: Final Assets = 113,755.16 USD, Reward = 9.24, Epsilon = 0.8919, Steps = 502\n",
      "Episode 12/200: Final Assets = 95,006.11 USD, Reward = 8.48, Epsilon = 0.8820, Steps = 502\n",
      "Episode 13/200: Final Assets = 111,738.45 USD, Reward = 12.23, Epsilon = 0.8720, Steps = 502\n",
      "Episode 14/200: Final Assets = 124,907.92 USD, Reward = 9.32, Epsilon = 0.8621, Steps = 502\n",
      "Episode 15/200: Final Assets = 111,549.84 USD, Reward = 9.83, Epsilon = 0.8522, Steps = 502\n",
      "Episode 16/200: Final Assets = 96,876.04 USD, Reward = 7.74, Epsilon = 0.8422, Steps = 502\n",
      "Episode 17/200: Final Assets = 108,549.49 USD, Reward = 9.17, Epsilon = 0.8323, Steps = 502\n",
      "Episode 18/200: Final Assets = 123,872.93 USD, Reward = 9.28, Epsilon = 0.8223, Steps = 502\n",
      "Episode 19/200: Final Assets = 100,076.85 USD, Reward = 7.01, Epsilon = 0.8124, Steps = 502\n",
      "Episode 20/200: Final Assets = 124,050.84 USD, Reward = 8.31, Epsilon = 0.8025, Steps = 502\n",
      "Episode 21/200: Final Assets = 139,318.60 USD, Reward = 13.17, Epsilon = 0.7925, Steps = 502\n",
      "Episode 22/200: Final Assets = 107,252.87 USD, Reward = 7.03, Epsilon = 0.7826, Steps = 502\n",
      "Episode 23/200: Final Assets = 110,368.82 USD, Reward = 11.11, Epsilon = 0.7726, Steps = 502\n",
      "Episode 24/200: Final Assets = 91,128.29 USD, Reward = 10.11, Epsilon = 0.7627, Steps = 502\n",
      "Episode 25/200: Final Assets = 132,748.61 USD, Reward = 10.62, Epsilon = 0.7528, Steps = 502\n",
      "Episode 26/200: Final Assets = 111,497.70 USD, Reward = 7.17, Epsilon = 0.7428, Steps = 502\n",
      "Episode 27/200: Final Assets = 101,211.19 USD, Reward = 8.63, Epsilon = 0.7329, Steps = 502\n",
      "Episode 28/200: Final Assets = 148,461.48 USD, Reward = 12.70, Epsilon = 0.7229, Steps = 502\n",
      "Episode 29/200: Final Assets = 114,958.25 USD, Reward = 10.38, Epsilon = 0.7130, Steps = 502\n",
      "Episode 30/200: Final Assets = 124,831.31 USD, Reward = 11.85, Epsilon = 0.7031, Steps = 502\n",
      "Episode 31/200: Final Assets = 119,987.49 USD, Reward = 11.17, Epsilon = 0.6931, Steps = 502\n",
      "Episode 32/200: Final Assets = 110,649.97 USD, Reward = 9.20, Epsilon = 0.6832, Steps = 502\n",
      "Episode 33/200: Final Assets = 146,846.11 USD, Reward = 11.63, Epsilon = 0.6732, Steps = 502\n",
      "Episode 34/200: Final Assets = 147,875.36 USD, Reward = 11.64, Epsilon = 0.6633, Steps = 502\n",
      "Episode 35/200: Final Assets = 109,979.12 USD, Reward = 10.20, Epsilon = 0.6534, Steps = 502\n",
      "Episode 36/200: Final Assets = 119,915.32 USD, Reward = 11.44, Epsilon = 0.6434, Steps = 502\n",
      "Episode 37/200: Final Assets = 124,460.01 USD, Reward = 11.99, Epsilon = 0.6335, Steps = 502\n",
      "Episode 38/200: Final Assets = 142,589.82 USD, Reward = 12.45, Epsilon = 0.6235, Steps = 502\n",
      "Episode 39/200: Final Assets = 80,682.14 USD, Reward = 6.20, Epsilon = 0.6136, Steps = 502\n",
      "Episode 40/200: Final Assets = 99,380.28 USD, Reward = 9.10, Epsilon = 0.6037, Steps = 502\n",
      "Episode 41/200: Final Assets = 114,085.27 USD, Reward = 10.67, Epsilon = 0.5937, Steps = 502\n",
      "Episode 42/200: Final Assets = 111,764.06 USD, Reward = 10.45, Epsilon = 0.5838, Steps = 502\n",
      "Episode 43/200: Final Assets = 128,945.69 USD, Reward = 13.51, Epsilon = 0.5738, Steps = 502\n",
      "Episode 44/200: Final Assets = 116,374.17 USD, Reward = 10.19, Epsilon = 0.5639, Steps = 502\n",
      "Episode 45/200: Final Assets = 144,709.15 USD, Reward = 12.67, Epsilon = 0.5540, Steps = 502\n",
      "Episode 46/200: Final Assets = 138,964.25 USD, Reward = 13.93, Epsilon = 0.5440, Steps = 502\n",
      "Episode 47/200: Final Assets = 110,999.08 USD, Reward = 9.81, Epsilon = 0.5341, Steps = 502\n",
      "Episode 48/200: Final Assets = 152,823.70 USD, Reward = 13.30, Epsilon = 0.5241, Steps = 502\n",
      "Episode 49/200: Final Assets = 107,681.05 USD, Reward = 12.22, Epsilon = 0.5142, Steps = 502\n",
      "Episode 50/200: Final Assets = 126,378.66 USD, Reward = 12.48, Epsilon = 0.5043, Steps = 502\n",
      "Episode 51/200: Final Assets = 108,162.45 USD, Reward = 9.57, Epsilon = 0.4943, Steps = 502\n",
      "Episode 52/200: Final Assets = 98,604.94 USD, Reward = 10.63, Epsilon = 0.4844, Steps = 502\n",
      "Episode 53/200: Final Assets = 95,222.88 USD, Reward = 9.11, Epsilon = 0.4744, Steps = 502\n",
      "Episode 54/200: Final Assets = 123,427.54 USD, Reward = 10.16, Epsilon = 0.4645, Steps = 502\n",
      "Episode 55/200: Final Assets = 123,418.10 USD, Reward = 11.49, Epsilon = 0.4546, Steps = 502\n",
      "Episode 56/200: Final Assets = 136,681.94 USD, Reward = 11.73, Epsilon = 0.4446, Steps = 502\n",
      "Episode 57/200: Final Assets = 124,456.44 USD, Reward = 10.23, Epsilon = 0.4347, Steps = 502\n",
      "Episode 58/200: Final Assets = 127,716.66 USD, Reward = 10.47, Epsilon = 0.4248, Steps = 502\n",
      "Episode 59/200: Final Assets = 99,898.61 USD, Reward = 8.89, Epsilon = 0.4148, Steps = 502\n",
      "Episode 60/200: Final Assets = 125,642.07 USD, Reward = 11.11, Epsilon = 0.4049, Steps = 502\n",
      "Episode 61/200: Final Assets = 156,652.44 USD, Reward = 14.56, Epsilon = 0.3949, Steps = 502\n",
      "Episode 62/200: Final Assets = 139,951.09 USD, Reward = 9.06, Epsilon = 0.3850, Steps = 502\n",
      "Episode 63/200: Final Assets = 109,816.17 USD, Reward = 9.62, Epsilon = 0.3751, Steps = 502\n",
      "Episode 64/200: Final Assets = 90,324.75 USD, Reward = 8.53, Epsilon = 0.3651, Steps = 502\n",
      "Episode 65/200: Final Assets = 108,361.06 USD, Reward = 7.03, Epsilon = 0.3552, Steps = 502\n",
      "Episode 66/200: Final Assets = 151,123.78 USD, Reward = 12.26, Epsilon = 0.3452, Steps = 502\n",
      "Episode 67/200: Final Assets = 130,203.87 USD, Reward = 10.74, Epsilon = 0.3353, Steps = 502\n",
      "Episode 68/200: Final Assets = 108,116.50 USD, Reward = 5.05, Epsilon = 0.3254, Steps = 502\n",
      "Episode 69/200: Final Assets = 212,835.48 USD, Reward = 14.72, Epsilon = 0.3154, Steps = 502\n",
      "Episode 70/200: Final Assets = 105,454.88 USD, Reward = 9.15, Epsilon = 0.3055, Steps = 502\n",
      "Episode 71/200: Final Assets = 114,281.60 USD, Reward = 10.47, Epsilon = 0.2955, Steps = 502\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 264\u001b[0m\n\u001b[0;32m    261\u001b[0m next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m    263\u001b[0m agent\u001b[38;5;241m.\u001b[39mremember(state, action, reward, next_state, done)\n\u001b[1;32m--> 264\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Learn after each step (or every few steps)\u001b[39;00m\n\u001b[0;32m    266\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m    267\u001b[0m episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[1;32mIn[1], line 210\u001b[0m, in \u001b[0;36mDQNAgent.learn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    207\u001b[0m         target_q_values[i][actions[i]] \u001b[38;5;241m=\u001b[39m rewards[i] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscount_factor \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mamax(next_q_values_target[i])\n\u001b[0;32m    209\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 210\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_q_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;66;03m# Decay epsilon\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon_min:\n",
      "File \u001b[1;32mc:\\Users\\Hi\\anaconda3\\envs\\all\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Hi\\anaconda3\\envs\\all\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:369\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    367\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_epoch_begin(epoch)\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m--> 369\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Hi\\anaconda3\\envs\\all\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:736\u001b[0m, in \u001b[0;36mTFEpochIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epoch_iterator)\n",
      "File \u001b[1;32mc:\\Users\\Hi\\anaconda3\\envs\\all\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:102\u001b[0m, in \u001b[0;36mEpochIterator._enumerate_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m steps_per_epoch \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_epoch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 102\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_iterator())\n\u001b[0;32m    103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_seen \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, steps_per_epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_execution):\n",
      "File \u001b[1;32mc:\\Users\\Hi\\anaconda3\\envs\\all\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:501\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[0;32m    500\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[1;32m--> 501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    503\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration in eager mode or within tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Hi\\anaconda3\\envs\\all\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:709\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    705\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot be specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 709\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_next_call_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Hi\\anaconda3\\envs\\all\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:748\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    745\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fulltype\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[0;32m    746\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_types)\n\u001b[0;32m    747\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39mexperimental_set_type(fulltype)\n\u001b[1;32m--> 748\u001b[0m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_variant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Hi\\anaconda3\\envs\\all\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3509\u001b[0m, in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   3508\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3509\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3510\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMakeIterator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3512\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# Re-define the StockTradingEnv class (as it was in the previous successful execution)\n",
    "class StockTradingEnv:\n",
    "    def __init__(self, df, initial_cash, buy_min_percent, buy_max_percent,\n",
    "                 sell_min_percent, sell_max_percent, transaction_fee_percent,\n",
    "                 transaction_session_limit, transaction_penalty,\n",
    "                 total_assets_loss_threshold, cash_loss_threshold,\n",
    "                 win_condition_total_assets):\n",
    "        self.df = df\n",
    "        self.initial_cash = initial_cash\n",
    "        self.cash = initial_cash\n",
    "        self.shares = 0\n",
    "        self.buy_min_percent = buy_min_percent\n",
    "        self.buy_max_percent = buy_max_percent\n",
    "        self.sell_min_percent = sell_min_percent\n",
    "        self.sell_max_percent = sell_max_percent\n",
    "        self.transaction_fee_percent = transaction_fee_percent\n",
    "        self.transaction_session_limit = transaction_session_limit\n",
    "        self.transaction_penalty = transaction_penalty\n",
    "        self.total_assets_loss_threshold = total_assets_loss_threshold\n",
    "        self.cash_loss_threshold = cash_loss_threshold\n",
    "        self.win_condition_total_assets = win_condition_total_assets\n",
    "        self.current_step = 0\n",
    "        self.no_transaction_count = 0\n",
    "        self.done = False\n",
    "        self.reward = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.cash = self.initial_cash\n",
    "        self.shares = 0\n",
    "        self.current_step = 0\n",
    "        self.no_transaction_count = 0\n",
    "        self.done = False\n",
    "        self.reward = 0\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        if self.current_step < len(self.df):\n",
    "            current_price = self.df.iloc[self.current_step]['Close']\n",
    "        else:\n",
    "            current_price = self.df.iloc[len(self.df) - 1]['Close'] # Use last known price if episode ended\n",
    "        # Normalize state features for better neural network performance\n",
    "        # It's good practice to normalize state features, e.g., by dividing by typical max values\n",
    "        # For simplicity, let's keep them raw for now, but note this as an improvement\n",
    "        return np.array([self.cash, self.shares, current_price], dtype=np.float32)\n",
    "\n",
    "    def _calculate_total_assets(self):\n",
    "        if self.current_step < len(self.df):\n",
    "            current_price = self.df.iloc[self.current_step]['Close']\n",
    "        else:\n",
    "            current_price = self.df.iloc[len(self.df) - 1]['Close'] # Use last known price if episode ended\n",
    "        return self.cash + (self.shares * current_price)\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.done or self.current_step >= len(self.df):\n",
    "            self.done = True\n",
    "            return self._get_state(), self.reward, self.done, {}\n",
    "\n",
    "        current_price = self.df.iloc[self.current_step]['Close']\n",
    "        \n",
    "        # Ensure current_price is valid to prevent division by zero or errors\n",
    "        if current_price <= 0:\n",
    "            self.reward = -5 # Penalty for invalid price data, or potentially end episode\n",
    "            self.done = True\n",
    "            return self._get_state(), self.reward, self.done, {\"message\": \"Invalid price data\"}\n",
    "\n",
    "        previous_total_assets = self._calculate_total_assets() # Calculate before action\n",
    "\n",
    "        # Apply no-transaction penalty\n",
    "        if action == 0: # Hold\n",
    "            self.no_transaction_count += 1\n",
    "            if self.no_transaction_count >= self.transaction_session_limit:\n",
    "                self.cash -= self.transaction_penalty\n",
    "                self.no_transaction_count = 0 # Reset count after applying penalty\n",
    "            self.reward = -0.001 # Small penalty for holding to encourage action\n",
    "        else:\n",
    "            self.no_transaction_count = 0\n",
    "\n",
    "        if action == 1:  # Buy\n",
    "            buy_amount_min = self.cash * self.buy_min_percent\n",
    "            buy_amount_max = self.cash * self.buy_max_percent\n",
    "            buy_value = min(buy_amount_max, self.cash)\n",
    "\n",
    "            if buy_value >= buy_amount_min and self.cash > 0:\n",
    "                shares_to_buy = int(buy_value / current_price)\n",
    "                cost = shares_to_buy * current_price\n",
    "                transaction_fee = cost * self.transaction_fee_percent\n",
    "                if self.cash >= (cost + transaction_fee) and shares_to_buy > 0:\n",
    "                    self.cash -= (cost + transaction_fee)\n",
    "                    self.shares += shares_to_buy\n",
    "                    self.reward = 0.1 # Small positive reward for buying\n",
    "                else:\n",
    "                    self.reward = -0.01 # Small penalty for failed transaction (e.g., not enough cash)\n",
    "            else:\n",
    "                self.reward = -0.01 # Small penalty for not meeting buy_min_percent or no cash\n",
    "\n",
    "        elif action == 2:  # Sell\n",
    "            sell_amount_min_shares = self.shares * self.sell_min_percent\n",
    "            sell_amount_max_shares = self.shares * self.sell_max_percent\n",
    "            shares_to_sell = min(int(sell_amount_max_shares), self.shares)\n",
    "\n",
    "            if shares_to_sell >= sell_amount_min_shares and self.shares > 0:\n",
    "                revenue = shares_to_sell * current_price\n",
    "                transaction_fee = revenue * self.transaction_fee_percent\n",
    "                self.cash += (revenue - transaction_fee)\n",
    "                self.shares -= shares_to_sell\n",
    "                self.reward = 0.1 # Small positive reward for selling\n",
    "            else:\n",
    "                self.reward = -0.01 # Small penalty for not meeting sell_min_percent or no shares\n",
    "\n",
    "        self.current_step += 1\n",
    "\n",
    "        # Check for termination conditions *after* incrementing step\n",
    "        if self.current_step >= len(self.df):\n",
    "            self.done = True\n",
    "            # Reward for end of episode based on final total assets\n",
    "            final_total_assets = self._calculate_total_assets()\n",
    "            self.reward += (final_total_assets - self.initial_cash) / self.initial_cash * 10 # Scale reward based on profit/loss\n",
    "        \n",
    "        # Check win/loss conditions (only if not already done by end of data)\n",
    "        if not self.done:\n",
    "            total_assets = self._calculate_total_assets()\n",
    "            if total_assets >= self.win_condition_total_assets:\n",
    "                self.reward = 100 # Large positive reward for winning\n",
    "                self.done = True\n",
    "            elif total_assets < self.total_assets_loss_threshold or self.cash < self.cash_loss_threshold:\n",
    "                self.reward = -100 # Large negative reward for losing\n",
    "                self.done = True\n",
    "        \n",
    "        next_state = self._get_state()\n",
    "        return next_state, self.reward, self.done, {}\n",
    "\n",
    "# --- DQN Agent Implementation ---\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, learning_rate=0.001, discount_factor=0.95,\n",
    "                 epsilon_start=1.0, epsilon_end=0.01, epsilon_decay_steps=5000,\n",
    "                 replay_buffer_size=10000, batch_size=32):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor  # Gamma\n",
    "        self.epsilon = epsilon_start            # Exploration rate\n",
    "        self.epsilon_min = epsilon_end\n",
    "        self.epsilon_decay_rate = (epsilon_start - epsilon_end) / epsilon_decay_steps\n",
    "\n",
    "        self.replay_buffer = deque(maxlen=replay_buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model() # Copy weights to target model\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Simple feedforward neural network\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Dense(64, activation='relu', input_shape=(self.state_size,)),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(self.action_size, activation='linear')\n",
    "        ])\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate),\n",
    "                      loss='mse')\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        # Reshape state for model prediction (add batch dimension)\n",
    "        q_values = self.model.predict(np.expand_dims(state, axis=0), verbose=0)[0]\n",
    "        return np.argmax(q_values)\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return # Not enough samples to learn\n",
    "\n",
    "        minibatch = random.sample(self.replay_buffer, self.batch_size)\n",
    "\n",
    "        states = np.array([t[0] for t in minibatch])\n",
    "        actions = np.array([t[1] for t in minibatch])\n",
    "        rewards = np.array([t[2] for t in minibatch])\n",
    "        next_states = np.array([t[3] for t in minibatch])\n",
    "        dones = np.array([t[4] for t in minibatch])\n",
    "\n",
    "        # Predict Q-values for current states\n",
    "        current_q_values = self.model.predict(states, verbose=0)\n",
    "        # Predict Q-values for next states using target model\n",
    "        next_q_values_target = self.target_model.predict(next_states, verbose=0)\n",
    "\n",
    "        # Calculate target Q-values\n",
    "        target_q_values = np.copy(current_q_values)\n",
    "        for i in range(self.batch_size):\n",
    "            if dones[i]:\n",
    "                target_q_values[i][actions[i]] = rewards[i]\n",
    "            else:\n",
    "                # Bellman equation: Q(s,a) = r + gamma * max(Q(s',a'))\n",
    "                target_q_values[i][actions[i]] = rewards[i] + self.discount_factor * np.amax(next_q_values_target[i])\n",
    "\n",
    "        # Train the model\n",
    "        self.model.fit(states, target_q_values, epochs=1, verbose=0)\n",
    "\n",
    "        # Decay epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon -= self.epsilon_decay_rate\n",
    "\n",
    "# --- Main Training Loop ---\n",
    "\n",
    "# Load the stock data\n",
    "stock_df = pd.read_csv('stock_2y.csv')\n",
    "\n",
    "# --- Environment Parameters (from previous problem) ---\n",
    "initial_cash = 100000\n",
    "buy_min_percent = 0.05\n",
    "buy_max_percent = 1.00\n",
    "sell_min_percent = 0.05\n",
    "sell_max_percent = 1.00\n",
    "transaction_fee_percent = 0.001\n",
    "transaction_session_limit = 5\n",
    "transaction_penalty = 100\n",
    "total_assets_loss_threshold = 10000\n",
    "cash_loss_threshold = -5000\n",
    "win_condition_total_assets = 1000000\n",
    "\n",
    "env = StockTradingEnv(stock_df, initial_cash, buy_min_percent, buy_max_percent,\n",
    "                      sell_min_percent, sell_max_percent, transaction_fee_percent,\n",
    "                      transaction_session_limit, transaction_penalty,\n",
    "                      total_assets_loss_threshold, cash_loss_threshold,\n",
    "                      win_condition_total_assets)\n",
    "\n",
    "state_size = env._get_state().shape[0] # Number of features in the state\n",
    "action_size = 3 # Hold, Buy, Sell\n",
    "\n",
    "agent = DQNAgent(state_size=state_size, action_size=action_size,\n",
    "                 learning_rate=0.0005, discount_factor=0.99, # Adjusted learning rate and discount for stability\n",
    "                 epsilon_start=1.0, epsilon_end=0.01, epsilon_decay_steps=50000, # More steps for decay\n",
    "                 replay_buffer_size=50000, batch_size=64) # Larger buffer and batch size\n",
    "\n",
    "num_episodes = 200 # Increased episodes for some learning to occur\n",
    "target_model_update_freq = 10 # Update target network every N episodes\n",
    "\n",
    "print(\"--- Starting DQN Training ---\")\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    step_count = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.choose_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        agent.learn() # Learn after each step (or every few steps)\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        step_count += 1\n",
    "    \n",
    "    # Update target model weights periodically\n",
    "    if episode % target_model_update_freq == 0:\n",
    "        agent.update_target_model()\n",
    "\n",
    "    final_total_assets = env._calculate_total_assets()\n",
    "    print(f\"Episode {episode + 1}/{num_episodes}: \"\n",
    "          f\"Final Assets = {final_total_assets:,.2f} USD, \"\n",
    "          f\"Reward = {episode_reward:,.2f}, \"\n",
    "          f\"Epsilon = {agent.epsilon:.4f}, \"\n",
    "          f\"Steps = {step_count}\")\n",
    "\n",
    "    # Optional: Save model weights after a certain performance or regularly\n",
    "    # if final_total_assets >= win_condition_total_assets:\n",
    "    #     print(f\"Goal achieved in episode {episode + 1}! Saving model...\")\n",
    "    #     agent.model.save(f\"stock_trader_dqn_win_ep{episode + 1}.h5\")\n",
    "    #     break # End training if goal is met\n",
    "\n",
    "print(\"\\n--- DQN Training Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
