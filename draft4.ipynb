{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbc47e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hi\\anaconda3\\envs\\all\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting DQN Training ---\n",
      "Episode 1/200: Final Assets = 105,980.29 USD, Reward = 8,017.49, Epsilon = 0.9913, Steps = 502\n",
      "Episode 2/200: Final Assets = 85,281.65 USD, Reward = 8,417.60, Epsilon = 0.9814, Steps = 502\n",
      "Episode 3/200: Final Assets = 129,063.53 USD, Reward = 25,549.87, Epsilon = 0.9714, Steps = 502\n",
      "Episode 4/200: Final Assets = 112,962.58 USD, Reward = -591.59, Epsilon = 0.9615, Steps = 502\n",
      "Episode 5/200: Final Assets = 101,427.12 USD, Reward = 9,251.63, Epsilon = 0.9515, Steps = 502\n",
      "Episode 6/200: Final Assets = 121,237.47 USD, Reward = 13,670.98, Epsilon = 0.9416, Steps = 502\n",
      "Episode 7/200: Final Assets = 109,494.23 USD, Reward = 3,662.20, Epsilon = 0.9317, Steps = 502\n",
      "Episode 8/200: Final Assets = 98,915.48 USD, Reward = -11,328.94, Epsilon = 0.9217, Steps = 502\n",
      "Episode 9/200: Final Assets = 101,612.42 USD, Reward = 24,333.66, Epsilon = 0.9118, Steps = 502\n",
      "Episode 10/200: Final Assets = 99,900.16 USD, Reward = -8,091.94, Epsilon = 0.9019, Steps = 502\n",
      "Episode 11/200: Final Assets = 111,737.44 USD, Reward = 14,414.68, Epsilon = 0.8919, Steps = 502\n",
      "Episode 12/200: Final Assets = 124,617.75 USD, Reward = 19,804.60, Epsilon = 0.8820, Steps = 502\n",
      "Episode 13/200: Final Assets = 127,021.91 USD, Reward = 17,226.22, Epsilon = 0.8720, Steps = 502\n",
      "Episode 14/200: Final Assets = 111,806.28 USD, Reward = 23,526.29, Epsilon = 0.8621, Steps = 502\n",
      "Episode 15/200: Final Assets = 118,734.03 USD, Reward = 39,187.51, Epsilon = 0.8522, Steps = 502\n",
      "Episode 16/200: Final Assets = 113,199.42 USD, Reward = 20,354.32, Epsilon = 0.8422, Steps = 502\n",
      "Episode 17/200: Final Assets = 100,804.64 USD, Reward = 8,166.71, Epsilon = 0.8323, Steps = 502\n",
      "Episode 18/200: Final Assets = 98,201.60 USD, Reward = 1,367.85, Epsilon = 0.8223, Steps = 502\n",
      "Episode 19/200: Final Assets = 102,106.62 USD, Reward = 15,496.91, Epsilon = 0.8124, Steps = 502\n",
      "Episode 20/200: Final Assets = 88,131.15 USD, Reward = 12,459.35, Epsilon = 0.8025, Steps = 502\n",
      "Episode 21/200: Final Assets = 115,903.36 USD, Reward = 30,979.91, Epsilon = 0.7925, Steps = 502\n",
      "Episode 22/200: Final Assets = 115,792.91 USD, Reward = 24,089.13, Epsilon = 0.7826, Steps = 502\n",
      "Episode 23/200: Final Assets = 110,387.75 USD, Reward = -1,740.04, Epsilon = 0.7726, Steps = 502\n",
      "Episode 24/200: Final Assets = 131,724.33 USD, Reward = 20,479.76, Epsilon = 0.7627, Steps = 502\n",
      "Episode 25/200: Final Assets = 130,889.42 USD, Reward = 36,735.79, Epsilon = 0.7528, Steps = 502\n",
      "Episode 26/200: Final Assets = 123,367.23 USD, Reward = 7,415.52, Epsilon = 0.7428, Steps = 502\n",
      "Episode 27/200: Final Assets = 122,173.92 USD, Reward = 32,638.41, Epsilon = 0.7329, Steps = 502\n",
      "Episode 28/200: Final Assets = 121,189.41 USD, Reward = 12,143.83, Epsilon = 0.7229, Steps = 502\n",
      "Episode 29/200: Final Assets = 117,107.58 USD, Reward = 19,490.48, Epsilon = 0.7130, Steps = 502\n",
      "Episode 30/200: Final Assets = 98,184.59 USD, Reward = 36,484.75, Epsilon = 0.7031, Steps = 502\n",
      "Episode 31/200: Final Assets = 111,366.94 USD, Reward = 32,814.18, Epsilon = 0.6931, Steps = 502\n",
      "Episode 32/200: Final Assets = 173,504.32 USD, Reward = 62,140.44, Epsilon = 0.6832, Steps = 502\n",
      "Episode 33/200: Final Assets = 102,310.90 USD, Reward = 11,076.36, Epsilon = 0.6732, Steps = 502\n",
      "Episode 34/200: Final Assets = 109,518.36 USD, Reward = 19,654.18, Epsilon = 0.6633, Steps = 502\n",
      "Episode 35/200: Final Assets = 144,400.66 USD, Reward = 47,123.40, Epsilon = 0.6534, Steps = 502\n",
      "Episode 36/200: Final Assets = 106,072.32 USD, Reward = 13,511.89, Epsilon = 0.6434, Steps = 502\n",
      "Episode 37/200: Final Assets = 98,772.96 USD, Reward = 9,263.48, Epsilon = 0.6335, Steps = 502\n",
      "Episode 38/200: Final Assets = 121,559.28 USD, Reward = 21,697.15, Epsilon = 0.6235, Steps = 502\n",
      "Episode 39/200: Final Assets = 92,279.88 USD, Reward = 29,638.07, Epsilon = 0.6136, Steps = 502\n",
      "Episode 40/200: Final Assets = 119,712.38 USD, Reward = 27,624.42, Epsilon = 0.6037, Steps = 502\n",
      "Episode 41/200: Final Assets = 109,963.88 USD, Reward = 31,024.61, Epsilon = 0.5937, Steps = 502\n",
      "Episode 42/200: Final Assets = 99,214.88 USD, Reward = 17,678.78, Epsilon = 0.5838, Steps = 502\n",
      "Episode 43/200: Final Assets = 98,770.77 USD, Reward = 37,951.80, Epsilon = 0.5738, Steps = 502\n",
      "Episode 44/200: Final Assets = 97,226.68 USD, Reward = 17,238.93, Epsilon = 0.5639, Steps = 502\n",
      "Episode 45/200: Final Assets = 100,986.57 USD, Reward = 27,084.45, Epsilon = 0.5540, Steps = 502\n",
      "Episode 46/200: Final Assets = 120,560.54 USD, Reward = 40,505.11, Epsilon = 0.5440, Steps = 502\n",
      "Episode 47/200: Final Assets = 137,132.92 USD, Reward = 43,391.76, Epsilon = 0.5341, Steps = 502\n",
      "Episode 48/200: Final Assets = 114,247.30 USD, Reward = 47,725.70, Epsilon = 0.5241, Steps = 502\n",
      "Episode 49/200: Final Assets = 100,959.61 USD, Reward = 34,637.29, Epsilon = 0.5142, Steps = 502\n",
      "Episode 50/200: Final Assets = 92,759.52 USD, Reward = 22,274.75, Epsilon = 0.5043, Steps = 502\n",
      "Episode 51/200: Final Assets = 100,912.71 USD, Reward = 41,822.85, Epsilon = 0.4943, Steps = 502\n",
      "Episode 52/200: Final Assets = 141,024.16 USD, Reward = 44,078.19, Epsilon = 0.4844, Steps = 502\n",
      "Episode 53/200: Final Assets = 125,810.71 USD, Reward = 37,361.85, Epsilon = 0.4744, Steps = 502\n",
      "Episode 54/200: Final Assets = 121,358.39 USD, Reward = 44,278.16, Epsilon = 0.4645, Steps = 502\n",
      "Episode 55/200: Final Assets = 103,768.59 USD, Reward = 45,359.91, Epsilon = 0.4546, Steps = 502\n",
      "Episode 56/200: Final Assets = 109,852.34 USD, Reward = 52,689.56, Epsilon = 0.4446, Steps = 502\n",
      "Episode 57/200: Final Assets = 97,871.04 USD, Reward = 30,659.07, Epsilon = 0.4347, Steps = 502\n",
      "Episode 58/200: Final Assets = 104,908.89 USD, Reward = 58,917.39, Epsilon = 0.4248, Steps = 502\n",
      "Episode 59/200: Final Assets = 121,270.72 USD, Reward = 62,876.98, Epsilon = 0.4148, Steps = 502\n",
      "Episode 60/200: Final Assets = 96,316.50 USD, Reward = 48,284.16, Epsilon = 0.4049, Steps = 502\n",
      "Episode 61/200: Final Assets = 113,496.11 USD, Reward = 60,311.26, Epsilon = 0.3949, Steps = 502\n",
      "Episode 62/200: Final Assets = 111,984.26 USD, Reward = 46,813.28, Epsilon = 0.3850, Steps = 502\n",
      "Episode 63/200: Final Assets = 88,684.90 USD, Reward = 40,042.82, Epsilon = 0.3751, Steps = 502\n",
      "Episode 64/200: Final Assets = 102,311.11 USD, Reward = 57,768.20, Epsilon = 0.3651, Steps = 502\n",
      "Episode 65/200: Final Assets = 78,358.07 USD, Reward = 27,057.11, Epsilon = 0.3552, Steps = 502\n",
      "Episode 66/200: Final Assets = 100,343.79 USD, Reward = 48,184.21, Epsilon = 0.3452, Steps = 502\n",
      "Episode 67/200: Final Assets = 124,866.50 USD, Reward = 46,661.93, Epsilon = 0.3353, Steps = 502\n",
      "Episode 68/200: Final Assets = 131,280.03 USD, Reward = 69,397.15, Epsilon = 0.3254, Steps = 502\n",
      "Episode 69/200: Final Assets = 122,403.93 USD, Reward = 82,092.46, Epsilon = 0.3154, Steps = 502\n",
      "Episode 70/200: Final Assets = 134,833.59 USD, Reward = 68,909.91, Epsilon = 0.3055, Steps = 502\n",
      "Episode 71/200: Final Assets = 149,902.60 USD, Reward = 74,271.23, Epsilon = 0.2955, Steps = 502\n",
      "Episode 72/200: Final Assets = 89,618.62 USD, Reward = 70,319.10, Epsilon = 0.2856, Steps = 502\n",
      "Episode 73/200: Final Assets = 113,219.73 USD, Reward = 74,598.86, Epsilon = 0.2757, Steps = 502\n",
      "Episode 74/200: Final Assets = 110,636.21 USD, Reward = 73,908.88, Epsilon = 0.2657, Steps = 502\n",
      "Episode 75/200: Final Assets = 110,508.53 USD, Reward = 66,028.27, Epsilon = 0.2558, Steps = 502\n",
      "Episode 76/200: Final Assets = 117,902.92 USD, Reward = 67,009.15, Epsilon = 0.2458, Steps = 502\n",
      "Episode 77/200: Final Assets = 109,442.55 USD, Reward = 70,913.38, Epsilon = 0.2359, Steps = 502\n",
      "Episode 78/200: Final Assets = 99,292.81 USD, Reward = 68,801.39, Epsilon = 0.2260, Steps = 502\n",
      "Episode 79/200: Final Assets = 137,370.27 USD, Reward = 92,490.90, Epsilon = 0.2160, Steps = 502\n",
      "Episode 80/200: Final Assets = 132,870.18 USD, Reward = 95,580.36, Epsilon = 0.2061, Steps = 502\n",
      "Episode 81/200: Final Assets = 133,531.64 USD, Reward = 87,994.84, Epsilon = 0.1961, Steps = 502\n",
      "Episode 82/200: Final Assets = 132,119.90 USD, Reward = 104,426.83, Epsilon = 0.1862, Steps = 502\n",
      "Episode 83/200: Final Assets = 122,988.26 USD, Reward = 105,914.48, Epsilon = 0.1763, Steps = 502\n",
      "Episode 84/200: Final Assets = 112,337.64 USD, Reward = 81,510.93, Epsilon = 0.1663, Steps = 502\n",
      "Episode 85/200: Final Assets = 124,554.18 USD, Reward = 83,356.27, Epsilon = 0.1564, Steps = 502\n",
      "Episode 86/200: Final Assets = 141,970.96 USD, Reward = 115,030.61, Epsilon = 0.1464, Steps = 502\n",
      "Episode 87/200: Final Assets = 129,048.49 USD, Reward = 107,548.33, Epsilon = 0.1365, Steps = 502\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 294\u001b[0m\n\u001b[0;32m    291\u001b[0m next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m    293\u001b[0m agent\u001b[38;5;241m.\u001b[39mremember(state, action, reward, next_state, done)\n\u001b[1;32m--> 294\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Learn after each step (or every few steps)\u001b[39;00m\n\u001b[0;32m    296\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m    297\u001b[0m episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[1;32mIn[1], line 222\u001b[0m, in \u001b[0;36mDQNAgent.learn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    220\u001b[0m current_q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(states, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    221\u001b[0m \u001b[38;5;66;03m# Predict Q-values for next states using target model\u001b[39;00m\n\u001b[1;32m--> 222\u001b[0m next_q_values_target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;66;03m# --- FASTER, VECTORIZED CALCULATION ---\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# Start with the current Q-values as a base\u001b[39;00m\n\u001b[0;32m    226\u001b[0m target_q_values \u001b[38;5;241m=\u001b[39m current_q_values\n",
      "File \u001b[1;32mc:\\Users\\Hi\\anaconda3\\envs\\all\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Hi\\anaconda3\\envs\\all\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:500\u001b[0m, in \u001b[0;36mTensorFlowTrainer.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;129m@traceback_utils\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_traceback\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28mself\u001b[39m, x, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    498\u001b[0m ):\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;66;03m# Create an iterator that yields batches of input data.\u001b[39;00m\n\u001b[1;32m--> 500\u001b[0m     epoch_iterator \u001b[38;5;241m=\u001b[39m \u001b[43mTFEpochIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdistribute_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps_per_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;66;03m# Container that configures and calls callbacks.\u001b[39;00m\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32mc:\\Users\\Hi\\anaconda3\\envs\\all\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:722\u001b[0m, in \u001b[0;36mTFEpochIterator.__init__\u001b[1;34m(self, distribute_strategy, *args, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    721\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribute_strategy \u001b[38;5;241m=\u001b[39m distribute_strategy\n\u001b[1;32m--> 722\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tf_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    723\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedDataset):\n\u001b[0;32m    724\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribute_strategy\u001b[38;5;241m.\u001b[39mexperimental_distribute_dataset(\n\u001b[0;32m    725\u001b[0m         dataset\n\u001b[0;32m    726\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Hi\\anaconda3\\envs\\all\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\array_data_adapter.py:140\u001b[0m, in \u001b[0;36mArrayDataAdapter.get_tf_dataset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m indices\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# We prefetch a single element. Computing large permutations can take\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# quite a while so we don't want to wait for prefetching over an epoch\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# boundary to trigger the next permutation. On the other hand, too many\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# simultaneous shuffles can contend on a hardware level and degrade all\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m# performance.\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m indices_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mindices_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpermutation\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mprefetch(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mslice_batch_indices\u001b[39m(indices):\n\u001b[0;32m    143\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert a Tensor of indices into a dataset of batched indices.\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \n\u001b[0;32m    145\u001b[0m \u001b[38;5;124;03m    This step can be accomplished in several ways. The most natural is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m        A Dataset of batched indices.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Hi\\anaconda3\\envs\\all\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:2341\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[1;34m(self, map_func, num_parallel_calls, deterministic, synchronous, use_unbounded_threadpool, name)\u001b[0m\n\u001b[0;32m   2336\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops -> map_op ->\u001b[39;00m\n\u001b[0;32m   2337\u001b[0m \u001b[38;5;66;03m# dataset_ops).\u001b[39;00m\n\u001b[0;32m   2338\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[0;32m   2339\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m map_op\n\u001b[1;32m-> 2341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmap_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_v2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2342\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2346\u001b[0m \u001b[43m    \u001b[49m\u001b[43msynchronous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynchronous\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2347\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_unbounded_threadpool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_unbounded_threadpool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2349\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Hi\\anaconda3\\envs\\all\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\map_op.py:43\u001b[0m, in \u001b[0;36m_map_v2\u001b[1;34m(input_dataset, map_func, num_parallel_calls, deterministic, synchronous, use_unbounded_threadpool, name)\u001b[0m\n\u001b[0;32m     38\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m deterministic \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m debug_mode\u001b[38;5;241m.\u001b[39mDEBUG_MODE:\n\u001b[0;32m     39\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `deterministic` argument has no effect unless the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`num_parallel_calls` argument is specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     42\u001b[0m     )\n\u001b[1;32m---> 43\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MapDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m      \u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpreserve_cardinality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m      \u001b[49m\u001b[43mforce_synchronous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msynchronous\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msynchronous\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m synchronous:\n",
      "File \u001b[1;32mc:\\Users\\Hi\\anaconda3\\envs\\all\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\map_op.py:164\u001b[0m, in \u001b[0;36m_MapDataset.__init__\u001b[1;34m(self, input_dataset, map_func, force_synchronous, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_force_synchronous \u001b[38;5;241m=\u001b[39m force_synchronous\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m name\n\u001b[1;32m--> 164\u001b[0m variant_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variant_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_func\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_func\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_inter_op_parallelism\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_use_inter_op_parallelism\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreserve_cardinality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preserve_cardinality\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_synchronous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_force_synchronous\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_common_args\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(input_dataset, variant_tensor)\n",
      "File \u001b[1;32mc:\\Users\\Hi\\anaconda3\\envs\\all\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3561\u001b[0m, in \u001b[0;36mmap_dataset\u001b[1;34m(input_dataset, other_arguments, f, output_types, output_shapes, use_inter_op_parallelism, preserve_cardinality, force_synchronous, metadata, name)\u001b[0m\n\u001b[0;32m   3559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   3560\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3561\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3562\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMapDataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother_arguments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3563\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_types\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_shapes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3564\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muse_inter_op_parallelism\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_inter_op_parallelism\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3565\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpreserve_cardinality\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve_cardinality\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforce_synchronous\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3566\u001b[0m \u001b[43m      \u001b[49m\u001b[43mforce_synchronous\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3568\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# Re-define the StockTradingEnv class (as it was in the previous successful execution)\n",
    "class StockTradingEnv:\n",
    "    def __init__(self, df, initial_cash, buy_min_percent, buy_max_percent,\n",
    "                 sell_min_percent, sell_max_percent, transaction_fee_percent,\n",
    "                 transaction_session_limit, transaction_penalty,\n",
    "                 total_assets_loss_threshold, cash_loss_threshold,\n",
    "                 win_condition_total_assets):\n",
    "        self.df = df\n",
    "        self.initial_cash = initial_cash\n",
    "        self.cash = initial_cash\n",
    "        self.shares = 0\n",
    "        self.buy_min_percent = buy_min_percent\n",
    "        self.buy_max_percent = buy_max_percent\n",
    "        self.sell_min_percent = sell_min_percent\n",
    "        self.sell_max_percent = sell_max_percent\n",
    "        self.transaction_fee_percent = transaction_fee_percent\n",
    "        self.transaction_session_limit = transaction_session_limit\n",
    "        self.transaction_penalty = transaction_penalty\n",
    "        self.total_assets_loss_threshold = total_assets_loss_threshold\n",
    "        self.cash_loss_threshold = cash_loss_threshold\n",
    "        self.win_condition_total_assets = win_condition_total_assets\n",
    "        self.current_step = 0\n",
    "        self.no_transaction_count = 0\n",
    "        self.done = False\n",
    "        self.reward = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.cash = self.initial_cash\n",
    "        self.shares = 0\n",
    "        self.current_step = 0\n",
    "        self.no_transaction_count = 0\n",
    "        self.done = False\n",
    "        self.reward = 0\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        # Define a lookback window\n",
    "        lookback_window = 30 # For example, look at the last 30 days\n",
    "        \n",
    "        # Ensure we don't go out of bounds at the start\n",
    "        start_index = max(0, self.current_step - lookback_window + 1)\n",
    "        end_index = self.current_step + 1\n",
    "\n",
    "        # Get the historical price data\n",
    "        price_history = self.df['Close'].iloc[start_index:end_index].values\n",
    "        \n",
    "        # Pad with the earliest price if history is shorter than the window\n",
    "        if len(price_history) < lookback_window:\n",
    "            padding = [price_history[0]] * (lookback_window - len(price_history))\n",
    "            price_history = np.concatenate([padding, price_history])\n",
    "\n",
    "        # Combine historical data with current portfolio status\n",
    "        # It's crucial to normalize these values!\n",
    "        normalized_cash = self.cash / self.initial_cash\n",
    "        normalized_shares = self.shares / 1000 # Assume max 1000 shares, adjust as needed\n",
    "        \n",
    "        # The state is now a combination of price history and current status\n",
    "        state = np.concatenate([[normalized_cash, normalized_shares], price_history])\n",
    "        return state\n",
    "\n",
    "    def _calculate_total_assets(self):\n",
    "        if self.current_step < len(self.df):\n",
    "            current_price = self.df.iloc[self.current_step]['Close']\n",
    "        else:\n",
    "            current_price = self.df.iloc[len(self.df) - 1]['Close'] # Use last known price if episode ended\n",
    "        return self.cash + (self.shares * current_price)\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.done or self.current_step >= len(self.df):\n",
    "            self.done = True\n",
    "            return self._get_state(), self.reward, self.done, {}\n",
    "\n",
    "        current_price = self.df.iloc[self.current_step]['Close']\n",
    "        \n",
    "        # Ensure current_price is valid to prevent division by zero or errors\n",
    "        if current_price <= 0:\n",
    "            self.reward = -5 # Penalty for invalid price data, or potentially end episode\n",
    "            self.done = True\n",
    "            return self._get_state(), self.reward, self.done, {\"message\": \"Invalid price data\"}\n",
    "\n",
    "        previous_total_assets = self._calculate_total_assets() # Calculate before action\n",
    "\n",
    "        # Apply no-transaction penalty\n",
    "        if action == 0: # Hold\n",
    "            self.no_transaction_count += 1\n",
    "            if self.no_transaction_count >= self.transaction_session_limit:\n",
    "                self.cash -= self.transaction_penalty\n",
    "                self.no_transaction_count = 0 # Reset count after applying penalty\n",
    "            self.reward = -0.001 # Small penalty for holding to encourage action\n",
    "        else:\n",
    "            self.no_transaction_count = 0\n",
    "\n",
    "        if action == 1:  # Buy\n",
    "            buy_amount_min = self.cash * self.buy_min_percent\n",
    "            buy_amount_max = self.cash * self.buy_max_percent\n",
    "            buy_value = min(buy_amount_max, self.cash)\n",
    "\n",
    "            if buy_value >= buy_amount_min and self.cash > 0:\n",
    "                shares_to_buy = int(buy_value / current_price)\n",
    "                cost = shares_to_buy * current_price\n",
    "                transaction_fee = cost * self.transaction_fee_percent\n",
    "                if self.cash >= (cost + transaction_fee) and shares_to_buy > 0:\n",
    "                    self.cash -= (cost + transaction_fee)\n",
    "                    self.shares += shares_to_buy\n",
    "                    self.reward = 0.1 # Small positive reward for buying\n",
    "                else:\n",
    "                    self.reward = -0.01 # Small penalty for failed transaction (e.g., not enough cash)\n",
    "            else:\n",
    "                self.reward = -0.01 # Small penalty for not meeting buy_min_percent or no cash\n",
    "\n",
    "        elif action == 2:  # Sell\n",
    "            sell_amount_min_shares = self.shares * self.sell_min_percent\n",
    "            sell_amount_max_shares = self.shares * self.sell_max_percent\n",
    "            shares_to_sell = min(int(sell_amount_max_shares), self.shares)\n",
    "\n",
    "            if shares_to_sell >= sell_amount_min_shares and self.shares > 0:\n",
    "                revenue = shares_to_sell * current_price\n",
    "                transaction_fee = revenue * self.transaction_fee_percent\n",
    "                \n",
    "                # To calculate profit, you need to track the average price you bought your shares at.\n",
    "                # (This requires adding `self.avg_buy_price` to your environment)\n",
    "                # For now, let's use a simplified profit calculation.\n",
    "                profit = (current_price - self.df.iloc[self.current_step-1]['Close']) * shares_to_sell\n",
    "                \n",
    "                self.cash += (revenue - transaction_fee)\n",
    "                self.shares -= shares_to_sell\n",
    "\n",
    "                # --- NEW REWARD ---\n",
    "                # The reward is now the profit itself. A good trade gets a positive reward, a bad one gets a negative reward.\n",
    "                self.reward = profit \n",
    "            else:\n",
    "                self.reward = -0.01 # Penalty for failed sell\n",
    "\n",
    "        self.current_step += 1\n",
    "\n",
    "        # Check for termination conditions *after* incrementing step\n",
    "        if self.current_step >= len(self.df):\n",
    "            self.done = True\n",
    "            # Reward for end of episode based on final total assets\n",
    "            final_total_assets = self._calculate_total_assets()\n",
    "            self.reward += (final_total_assets - self.initial_cash) / self.initial_cash * 10 # Scale reward based on profit/loss\n",
    "        \n",
    "        # Check win/loss conditions (only if not already done by end of data)\n",
    "        if not self.done:\n",
    "            total_assets = self._calculate_total_assets()\n",
    "            if total_assets >= self.win_condition_total_assets:\n",
    "                self.reward = 100 # Large positive reward for winning\n",
    "                self.done = True\n",
    "            elif total_assets < self.total_assets_loss_threshold or self.cash < self.cash_loss_threshold:\n",
    "                self.reward = -100 # Large negative reward for losing\n",
    "                self.done = True\n",
    "        \n",
    "        next_state = self._get_state()\n",
    "        return next_state, self.reward, self.done, {}\n",
    "\n",
    "# --- DQN Agent Implementation ---\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, learning_rate=0.001, discount_factor=0.95,\n",
    "                 epsilon_start=1.0, epsilon_end=0.01, epsilon_decay_steps=5000,\n",
    "                 replay_buffer_size=10000, batch_size=32):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor  # Gamma\n",
    "        self.epsilon = epsilon_start            # Exploration rate\n",
    "        self.epsilon_min = epsilon_end\n",
    "        self.epsilon_decay_rate = (epsilon_start - epsilon_end) / epsilon_decay_steps\n",
    "\n",
    "        self.replay_buffer = deque(maxlen=replay_buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model() # Copy weights to target model\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Simple feedforward neural network\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Dense(64, activation='relu', input_shape=(self.state_size,)),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(self.action_size, activation='linear')\n",
    "        ])\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate),\n",
    "                      loss='mse')\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        # Reshape state for model prediction (add batch dimension)\n",
    "        q_values = self.model.predict(np.expand_dims(state, axis=0), verbose=0)[0]\n",
    "        return np.argmax(q_values)\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.replay_buffer, self.batch_size)\n",
    "\n",
    "        states = np.array([t[0] for t in minibatch])\n",
    "        actions = np.array([t[1] for t in minibatch])\n",
    "        rewards = np.array([t[2] for t in minibatch])\n",
    "        next_states = np.array([t[3] for t in minibatch])\n",
    "        dones = np.array([t[4] for t in minibatch], dtype=np.uint8) # Use boolean or uint8\n",
    "\n",
    "        # Predict Q-values for current states\n",
    "        current_q_values = self.model.predict(states, verbose=0)\n",
    "        # Predict Q-values for next states using target model\n",
    "        next_q_values_target = self.target_model.predict(next_states, verbose=0)\n",
    "        \n",
    "        # --- FASTER, VECTORIZED CALCULATION ---\n",
    "        # Start with the current Q-values as a base\n",
    "        target_q_values = current_q_values\n",
    "        \n",
    "        # Get the max Q-value for the next state (the 'max(Q(s',a'))' part)\n",
    "        max_next_q = np.amax(next_q_values_target, axis=1)\n",
    "        \n",
    "        # Calculate the updated Q-value using the Bellman equation\n",
    "        # (1 - dones) will be 1 for non-terminal states and 0 for terminal states\n",
    "        updated_q = rewards + self.discount_factor * max_next_q * (1 - dones)\n",
    "        \n",
    "        # Update the Q-value for the action that was actually taken\n",
    "        # This is an elegant way to replace the loop\n",
    "        target_q_values[range(self.batch_size), actions] = updated_q\n",
    "\n",
    "        # Train the model\n",
    "        self.model.fit(states, target_q_values, epochs=1, verbose=0)\n",
    "\n",
    "        # Decay epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon -= self.epsilon_decay_rate\n",
    "\n",
    "# --- Main Training Loop ---\n",
    "\n",
    "# Load the stock data\n",
    "stock_df = pd.read_csv('stock_2y.csv')\n",
    "\n",
    "# --- Environment Parameters (from previous problem) ---\n",
    "initial_cash = 100000\n",
    "buy_min_percent = 0.05\n",
    "buy_max_percent = 1.00\n",
    "sell_min_percent = 0.05\n",
    "sell_max_percent = 1.00\n",
    "transaction_fee_percent = 0.001\n",
    "transaction_session_limit = 5\n",
    "transaction_penalty = 100\n",
    "total_assets_loss_threshold = 10000\n",
    "cash_loss_threshold = -5000\n",
    "win_condition_total_assets = 1000000\n",
    "\n",
    "env = StockTradingEnv(stock_df, initial_cash, buy_min_percent, buy_max_percent,\n",
    "                      sell_min_percent, sell_max_percent, transaction_fee_percent,\n",
    "                      transaction_session_limit, transaction_penalty,\n",
    "                      total_assets_loss_threshold, cash_loss_threshold,\n",
    "                      win_condition_total_assets)\n",
    "\n",
    "state_size = env._get_state().shape[0] # Number of features in the state\n",
    "action_size = 3 # Hold, Buy, Sell\n",
    "\n",
    "agent = DQNAgent(state_size=state_size, action_size=action_size,\n",
    "                 learning_rate=0.0005, discount_factor=0.99, # Adjusted learning rate and discount for stability\n",
    "                 epsilon_start=1.0, epsilon_end=0.01, epsilon_decay_steps=50000, # More steps for decay\n",
    "                 replay_buffer_size=50000, batch_size=64) # Larger buffer and batch size\n",
    "\n",
    "num_episodes = 200 # Increased episodes for some learning to occur\n",
    "target_model_update_freq = 10 # Update target network every N episodes\n",
    "\n",
    "print(\"--- Starting DQN Training ---\")\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    step_count = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.choose_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        agent.learn() # Learn after each step (or every few steps)\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        step_count += 1\n",
    "    \n",
    "    # Update target model weights periodically\n",
    "    if episode % target_model_update_freq == 0:\n",
    "        agent.update_target_model()\n",
    "\n",
    "    final_total_assets = env._calculate_total_assets()\n",
    "    print(f\"Episode {episode + 1}/{num_episodes}: \"\n",
    "          f\"Final Assets = {final_total_assets:,.2f} USD, \"\n",
    "          f\"Reward = {episode_reward:,.2f}, \"\n",
    "          f\"Epsilon = {agent.epsilon:.4f}, \"\n",
    "          f\"Steps = {step_count}\")\n",
    "\n",
    "    # Optional: Save model weights after a certain performance or regularly\n",
    "    # if final_total_assets >= win_condition_total_assets:\n",
    "    #     print(f\"Goal achieved in episode {episode + 1}! Saving model...\")\n",
    "    #     agent.model.save(f\"stock_trader_dqn_win_ep{episode + 1}.h5\")\n",
    "    #     break # End training if goal is met\n",
    "\n",
    "print(\"\\n--- DQN Training Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
